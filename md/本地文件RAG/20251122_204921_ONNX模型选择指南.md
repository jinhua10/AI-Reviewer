# 🤖 ONNX 模型选择指南

## 📋 模型文件说明

`paraphrase-multilingual-MiniLM-L12-v2` 提供了多个优化版本：

### 1. **model.onnx** ⭐ 推荐
- **类型：** 标准未优化版本
- **精度：** FP32（32位浮点）
- **大小：** ~280MB
- **速度：** 基准速度
- **兼容性：** ✅ 所有平台通用
- **推荐场景：** 开发、测试、通用部署

### 2. **model_O1.onnx** / **model_O2.onnx** / **model_O3.onnx** / **model_O4.onnx**
- **类型：** ONNX Runtime 优化级别 1-4
- **说明：** 
  - O1: 基本优化（图融合）
  - O2: 扩展优化（常量折叠）
  - O3: 布局优化
  - O4: 所有优化（可能不稳定）
- **推荐：** **model_O2.onnx** 或 **model_O3.onnx**（平衡性能和稳定性）

### 3. **量化模型（Quantized）**

#### model_qint8_arm64.onnx
- **平台：** ARM64 架构（Apple M1/M2, ARM服务器）
- **精度：** INT8（8位整数）
- **大小：** ~70MB
- **速度：** 快（ARM设备上）
- **推荐：** ✅ 如果在 Mac M1/M2 上

#### model_qint8_avx512.onnx
- **平台：** Intel CPU 支持 AVX-512
- **精度：** INT8
- **大小：** ~70MB
- **速度：** 非常快
- **推荐：** ✅ 如果 CPU 支持 AVX-512

#### model_qint8_avx512_vnni.onnx
- **平台：** Intel CPU 支持 AVX-512 + VNNI
- **精度：** INT8
- **速度：** 最快
- **推荐：** ✅ 如果 CPU 支持（Intel Ice Lake 及更新）

#### model_quint8_avx2.onnx
- **平台：** Intel/AMD CPU 支持 AVX2
- **精度：** UINT8
- **大小：** ~70MB
- **速度：** 快
- **推荐：** ✅ 大多数现代 CPU

---

## 🎯 选择建议

### 场景 1: 开发和测试（推荐）

```
使用: model.onnx
原因: 
✅ 通用兼容性最好
✅ 不需要检查硬件特性
✅ 精度最高
✅ 问题排查简单
```

### 场景 2: 生产环境 - Windows/Linux 服务器

#### 选项 A: 优先性能（推荐）
```
1. 检查 CPU 特性
2. 选择对应的量化模型:
   - 支持 AVX-512 VNNI → model_qint8_avx512_vnni.onnx ⭐⭐⭐
   - 支持 AVX-512 → model_qint8_avx512.onnx ⭐⭐
   - 支持 AVX2 → model_quint8_avx2.onnx ⭐
   - 都不支持 → model_O2.onnx
```

#### 选项 B: 优先稳定（保守）
```
使用: model_O2.onnx 或 model_O3.onnx
原因: 适度优化，稳定性好
```

### 场景 3: Mac M1/M2

```
使用: model_qint8_arm64.onnx
原因: 
✅ 专门针对 ARM64 优化
✅ 4倍速度提升
✅ 内存占用减少 75%
```

### 场景 4: 内存受限环境

```
使用: 任何量化模型 (qint8/quint8)
原因:
✅ 大小从 280MB → 70MB
✅ 推理速度提升 2-4倍
⚠️ 精度略有下降（通常可忽略）
```

---

## 💻 检查你的 CPU 特性

### Windows PowerShell
```powershell
# 检查 CPU 信息
Get-WmiObject Win32_Processor | Select-Object Name

# 检查支持的指令集（需要额外工具）
# 推荐使用: CPU-Z 或 HWiNFO
```

### Linux
```bash
# 检查 CPU 特性
lscpu | grep -E "Model name|Flags"

# 检查 AVX 支持
grep -o 'avx[^ ]*' /proc/cpuinfo | sort -u
```

### 快速判断
```bash
# AVX2 (2013+): 大多数现代 Intel/AMD CPU
# AVX-512 (2017+): Intel Skylake-X, Ice Lake 及更新
# AVX-512 VNNI (2019+): Intel Ice Lake, Tiger Lake 及更新
```

---

## 📊 性能对比

| 模型 | 大小 | 速度 | 精度 | 推荐场景 |
|------|------|------|------|---------|
| model.onnx | 280MB | 1x (基准) | 100% | 开发/测试 ⭐⭐⭐ |
| model_O2.onnx | 280MB | 1.2x | 100% | 生产（稳定） ⭐⭐ |
| model_O3.onnx | 280MB | 1.3x | 100% | 生产（性能） ⭐⭐ |
| model_quint8_avx2.onnx | 70MB | 2-3x | 99.5% | 现代CPU ⭐⭐⭐ |
| model_qint8_avx512.onnx | 70MB | 3-4x | 99.5% | Intel服务器 ⭐⭐⭐ |
| model_qint8_avx512_vnni.onnx | 70MB | 4-5x | 99.5% | 新Intel CPU ⭐⭐⭐⭐ |
| model_qint8_arm64.onnx | 70MB | 3-4x | 99.5% | Mac M1/M2 ⭐⭐⭐⭐ |

---

## 🎯 我的推荐

### 对于你的项目（Excel 知识库问答系统）

#### 推荐方案 1: 简单通用（⭐⭐⭐ 推荐）
```
使用: model.onnx
路径: ./models/paraphrase-multilingual/model.onnx

优点:
✅ 无需检查硬件
✅ 兼容性最好
✅ 精度最高
✅ 开发测试都方便

适合:
- 初期开发
- 快速上手
- 硬件环境不确定
```

#### 推荐方案 2: 性能优化（⭐⭐ 如果需要速度）
```
使用: model_quint8_avx2.onnx
路径: ./models/paraphrase-multilingual/model_quint8_avx2.onnx

优点:
✅ 速度提升 2-3倍
✅ 内存减少 75%
✅ 大多数 CPU 支持（AVX2）
✅ 精度损失可忽略

适合:
- 生产环境
- 需要处理大量数据
- 2013年后的 CPU
```

#### 推荐方案 3: 极致性能（⭐ 高级用户）
```
检测 CPU 并选择最优模型:
1. 检查 CPU 特性
2. 选择对应的量化模型
3. 回退到 model.onnx（如果不支持）

优点:
✅ 最佳性能
✅ 最优资源利用

适合:
- 生产环境优化
- 明确硬件配置
- 需要极致性能
```

---

## 🔧 配置示例

### 方案 1: 使用标准模型（推荐）

下载模型时只下载：
```bash
models/paraphrase-multilingual/
  ├── model.onnx          # ← 只需要这个
  ├── config.json
  └── tokenizer.json
```

无需修改代码，现有配置已支持。

### 方案 2: 使用量化模型

下载模型时：
```bash
models/paraphrase-multilingual/
  ├── model_quint8_avx2.onnx   # ← 下载这个
  ├── config.json
  └── tokenizer.json
```

修改代码（如果需要指定文件名）：
```java
// 在 OptimizedExcelKnowledgeBuilder.java
String resourcePath = "/models/paraphrase-multilingual/model_quint8_avx2.onnx";
String fileSystemPath = "./models/paraphrase-multilingual/model_quint8_avx2.onnx";
```

或者更简单：**将量化模型重命名为 model.onnx**（推荐）

---

## ⚡ 快速决策流程图

```
开始
  ↓
是否需要优化性能？
  ├─ 否 → 使用 model.onnx ✅ （最简单）
  └─ 是 ↓
      ↓
  你的平台是？
      ├─ Mac M1/M2 → model_qint8_arm64.onnx ✅
      ├─ Intel/AMD → 继续 ↓
      └─ ARM服务器 → model_qint8_arm64.onnx ✅
          ↓
  CPU 支持什么指令集？
      ├─ AVX-512 VNNI → model_qint8_avx512_vnni.onnx ⭐⭐⭐⭐
      ├─ AVX-512 → model_qint8_avx512.onnx ⭐⭐⭐
      ├─ AVX2 → model_quint8_avx2.onnx ⭐⭐⭐
      └─ 不确定 → model.onnx ✅ （保险选择）
```

---

## 💡 实用建议

### 第一步：从简单开始
```
1. 先使用 model.onnx
2. 确保系统正常工作
3. 如果性能足够，就不需要优化
```

### 第二步：性能测试
```
如果性能不足：
1. 尝试 model_quint8_avx2.onnx（最安全的量化）
2. 对比速度和精度
3. 如果满意就使用
```

### 第三步：极致优化（可选）
```
如果还需要更快：
1. 检查 CPU 特性
2. 尝试更高级的优化版本
3. 测试稳定性
```

---

## ✅ 总结

### 我的最终推荐

**对于你的项目（Excel 知识库问答系统）：**

#### 🥇 首选：model.onnx
```
优点: 简单、稳定、兼容性好
适合: 95% 的使用场景
```

#### 🥈 备选：model_quint8_avx2.onnx
```
优点: 性能提升 2-3倍，兼容大多数 CPU
适合: 需要处理大量 Excel 文件
使用方法: 下载后重命名为 model.onnx
```

#### 🥉 高级：根据 CPU 选择
```
优点: 最优性能
适合: 生产环境优化
需要: 了解硬件配置
```

### 当前建议

**直接使用 model.onnx**，原因：
1. ✅ 你的代码已经配置好
2. ✅ 不需要修改任何代码
3. ✅ 通用性最好
4. ✅ 足够快（向量嵌入通常很快）

如果以后发现性能瓶颈，再考虑切换到量化模型。

---

需要我帮你配置特定的模型版本吗？


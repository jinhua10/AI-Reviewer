# 提示词改进说明 / Prompt Improvement Summary

## 📋 改进概览

根据 `prompt-analysis.md` 的分析，我对评审提示词进行了全面优化，以解决AI给分过高的问题。

### 主要改进内容

---

## 🔍 1. 新增强制性预评分检查（STEP 0）

### 问题
原版提示词缺少系统性的代码审查流程，AI容易忽略安全漏洞和质量问题。

### 改进
添加了**步骤0 - 安全审计**，要求AI在评分前必须：

#### 安全检查项：
- ✅ 搜索硬编码凭证（password, api_key, secret, token）
- ✅ 检查SQL注入风险
- ✅ 检查XSS漏洞
- ✅ 检查路径遍历漏洞
- ✅ 检查不安全的反序列化

#### 质量检查项：
- ✅ 是否有测试文件（test/, tests/）
- ✅ 是否有日志系统（logging, log4j）
- ✅ 是否有配置管理（.env, config.yaml）
- ✅ 异常处理质量（通用/具体）

---

## ⚖️ 2. 明确的自动扣分机制

### 问题
原版评分标准模糊，AI难以判断何时应该扣分。

### 改进
建立了**强制扣分清单**：

| 问题 | 扣分 | 从哪项扣 |
|------|------|----------|
| 硬编码API密钥/密码 | -8至-10分 | 安全性 |
| 无测试文件 | -5分 | 代码规范 |
| 无日志系统 | -3分 | 代码规范 |
| 仅通用异常捕获 | -2分 | 安全性 |
| 无配置管理 | -2分 | 技术实现 |
| SQL注入风险 | -5至-7分 | 安全性 |
| XSS漏洞 | -5至-7分 | 安全性 |
| 无输入验证 | -3至-4分 | 安全性 |

---

## 📊 3. 量化的创新性评分标准

### 问题
原版标准"高度原创"vs"良好创意"边界不清，AI容易高估创新性。

### 改进
建立了**分层基础分制度**：

#### 基础分判定：
```
• 新颖算法/架构（现有方案中未见）：基础分 15-17/20
• 重大改进（有可衡量收益）：基础分 12-14/20
• 标准方案+少量定制：基础分 8-10/20
• 直接使用现有库/框架：基础分 6-8/20
• 复制教程/模板：基础分 0-5/20
```

#### 调整因子：
- **加分**：独特解决方案(+2-3)、原创特性(+1-2)
- **减分**：仅组合工具无洞察(-2-3)

**示例应用**：
- RAG+文档问答项目：基础分6-8分（直接使用LangChain+FAISS）→ 减2分（仅组合工具）→ 最终4-6分

---

## 🏗️ 4. 技术实现的质量检查清单

### 问题
原版只有模糊的"优秀代码质量"描述。

### 改进
添加了**必需的质量检查**：

#### 扣分项：
- 缺少单元测试：-3至-4分
- 无错误处理或仅通用捕获：-2至-3分
- 代码重复（DRY违规）：-1至-2分
- 紧耦合难扩展：-2至-3分

#### 加分项：
- 正确使用设计模式：+2至+3分
- 清晰架构（分层/模块化）：+1至+2分

---

## 🎯 5. 完整性验证机制

### 问题
原版未要求对照README验证功能。

### 改进
要求AI：
1. **列出README声明的所有功能**
2. **验证每个功能是否工作**
3. **计算完成百分比**
4. **基于实际完成度打分**

评分档次：
- 100%功能工作 → 16-18分
- 80%+工作 → 13-15分
- 60-80%工作 → 10-12分
- 40-60%工作 → 7-9分
- <40%工作 → 0-6分

---

## 💼 6. 实用性的部署考量

### 问题
原版未考虑实际部署难度和成本。

### 改进
添加了**部署考虑因素**：

#### 扣分项：
- 需要付费云服务（AWS/GCP/Azure）：-1至-2分
- 无本地/开源替代：-1分
- 复杂部署（>5步）：-1至-2分
- 缺少部署文档：-1分

#### 加分项：
- 提供Docker/易部署：+1至+2分
- 离线工作/无外部依赖：+1分

**示例应用**：
- 依赖AWS Bedrock的项目：基础10分 → -2分（付费服务）→ -1分（无本地替代）→ 最终7分

---

## 📝 7. 代码规范的强制检查

### 问题
原版未强制检查测试和日志。

### 改进
**强制扣分项（必须检查）**：
- ✅ 无测试文件：扣3分
- ✅ 无日志：扣2分
- ✅ 无类型提示/注解：扣1分
- ✅ 无README：扣1-2分
- ✅ 操纵性注释：扣2-3分

---

## 🔐 8. 安全审计结果前置

### 问题
原版安全评分缺乏具体检查项。

### 改进
要求AI在输出开头必须先给出**安全审计结果**：

```
【安全审计结果】（必须首先完成）
⚠️ 发现的关键问题：
- 硬编码凭证：[是/否] - [详情及文件位置]
- SQL注入风险：[是/否] - [详情]
- XSS漏洞：[是/否] - [详情]

✅ 质量检查：
- 存在测试文件：[是/否] - [列出]
- 日志系统：[是/否] - [使用哪个库]
- 配置管理：[是/否] - [.env等]
```

---

## 📋 9. 详细的评分说明格式

### 问题
原版只要求简短理由，缺乏透明度。

### 改进
每项分数必须详细说明：

```
- 创新性：X/20分（最高：20）
  • 基础分：X/20 - [属于哪一档]
  • 调整：[列出每个+/-及原因]
  • 最终：X/20
  • 证据：[具体代码示例]
```

---

## 🎯 10. 新增"关键问题"和优先级

### 问题
原版未区分问题严重程度。

### 改进
新增两个部分：

#### 【关键问题】（如有）
列出必须修复的安全/质量问题及位置

#### 【改进建议】
每条建议标注优先级（高/中/低）

---

## 📊 预期效果对比

### 原版提示词可能导致的评分：
```
创新性：14-17/20 ❌ 过高
技术实现：16-18/20 ❌ 忽略缺少测试
完整性：17-19/20 ❌ 未验证功能
实用性：11-13/15 ❌ 未考虑部署难度
代码规范：7-9/10 ❌ 未检查日志
安全性：10-12/15 ❌ 忽略硬编码密钥

总分：75-88/100 ❌ 偏高
```

### 改进版提示词预期评分（同一RAG项目）：
```
创新性：6-8/20 ✅ 识别为"直接使用现有库"
技术实现：10-12/20 ✅ 扣除"无测试"(-4分)
完整性：13-15/20 ✅ 对照README验证
实用性：7-9/15 ✅ 扣除"需AWS付费"(-2分)
代码规范：2-4/10 ✅ 扣除"无测试"(-3)+"无日志"(-2)
安全性：2-5/15 ✅ 扣除"硬编码密钥"(-10分)

总分：40-53/100 ✅ 更严格且客观
```

**分数差异**：约25-35分 ⬇️

---

## 📁 文件位置

### 英文版（用于application.yml）
- **位置**：`application-demo/hackathonApplication/src/main/resources/application.yml`
- **字段**：`ai-engine.llm.user-prompt`
- **占位符**：`%s` （用于插入项目代码）

### 中文版（独立文件）
- **位置**：`application-demo/hackathonApplication/提示词.txt`
- **用途**：供参考或用于中文评审场景
- **占位符**：`%s` （用于插入项目代码）

---

## 🔧 使用说明

### 配置已自动更新
无需手动修改，application.yml中的`user-prompt`已更新为改进版。

### 调整评分严格度
如需调整，可修改：
1. **基础分范围**（如创新性的6-8分可调为8-10分）
2. **扣分力度**（如硬编码密钥从-8至-10改为-5至-7）
3. **top-p参数**（当前0.3极严格，可调为0.5-0.7）

### 验证效果
使用相同项目测试改进前后的评分差异。

---

## ✅ 改进总结

| 维度 | 原版问题 | 改进方案 | 效果 |
|------|----------|----------|------|
| 安全审计 | 无系统检查 | 强制STEP 0检查 | 发现硬编码密钥等问题 |
| 创新性 | 标准模糊 | 分层基础分+调整因子 | 降低虚高评分15-20分 |
| 技术实现 | 忽略测试 | 强制检查清单 | 扣除3-4分 |
| 完整性 | 未验证功能 | 对照README验证 | 更真实反映完成度 |
| 实用性 | 未考虑部署 | 部署考量因子 | 扣除2-4分 |
| 代码规范 | 检查不严 | 强制扣分项 | 扣除5分（无测试+日志） |
| 安全性 | 容易忽略 | 自动扣分机制 | 扣除8-10分（硬编码密钥） |
| 透明度 | 评分理由简单 | 详细评分说明 | 提高可信度 |

**总体效果**：预计平均降低评分**20-30分**，使评分更客观、严格、可追溯。

---

## 🚀 后续优化建议

1. **收集实际评分数据**，统计改进前后的分数分布
2. **A/B测试**：对比原版和改进版对相同项目的评分
3. **持续调优**：根据实际效果微调基础分和扣分力度
4. **添加更多检查项**：如代码复杂度、性能问题等
5. **机器学习**：基于历史评审数据训练更精准的评分模型

---

生成时间：2025-11-28
基于分析：prompt-analysis.md

